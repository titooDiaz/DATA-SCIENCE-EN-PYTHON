{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/CV/2_Convoluciones/Capas_Convolucionales.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>\n"
      ],
      "metadata": {
        "id": "TVi72lWrGL_V"
      },
      "id": "TVi72lWrGL_V"
    },
    {
      "cell_type": "markdown",
      "id": "7ac0440a",
      "metadata": {
        "origin_pos": 1,
        "id": "7ac0440a"
      },
      "source": [
        "# Redes neuronales convolucionales.\n",
        "\n",
        "En la clase anterior vimos las convoluciones eran una herramienta muy útil en procesamiento de imágenes. La gran utilidad que tienen estas herramientas eventualmente dio lugar a crear eventualmente a las redes neuronales convolucionales. La diferencia entre estas redes y los MLP que habíamos visto anteriormente es que aquí los parametro  a ser aprendidos son los kernels que realizan las convoluciones.\n",
        "\n",
        "Veamos un ejemplo de esto empezando con un ejemplo de una cuadricula de $3 \\times 3$ a la que le aplicamos un kernel de $ \\times 2$\n",
        "\n",
        "\n",
        "![](http://d2l.ai/_images/correlation.svg)\n",
        "\n",
        "\n",
        "Analicemos como hemos llegado al tamaño final \n",
        "\n",
        "$$\n",
        "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n",
        "1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n",
        "3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n",
        "4\\times0+5\\times1+7\\times2+8\\times3=43.\n",
        "$$\n",
        "\n",
        "Tenga en cuenta que a lo largo de cada eje, el tamaño de salida es ligeramente más pequeño que el tamaño de entrada. Debido a que el kernel tiene ancho y alto mayor que uno, solo podemos calcular correctamente la convolución para ubicaciones donde el kernel encaja completamente dentro de la imagen, el tamaño de salida viene dado por el tamaño de entrada $n_h \\times n_w$ menos el tamaño del kernel de convolución $k_h \\times k_w$ a través de\n",
        "\n",
        "$$(n_h-k_h+1) \\times (n_w-k_w+1).$$\n",
        "\n",
        "Este es el caso ya que necesitamos suficiente espacio para \"mover\" el kernel de convolución a través de la imagen. Más adelante veremos cómo mantener el tamaño sin cambios. rellenando la imagen con ceros alrededor de su borde para que haya suficiente espacio para el kernel. A continuación, implementamos este proceso en la función `corr2d`, que acepta un tensor de entrada `X` y un tensor kernel `K` y devuelve un tensor de salida `Y`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a68ca40a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:10.965645Z",
          "iopub.status.busy": "2022-09-07T22:11:10.965094Z",
          "iopub.status.idle": "2022-09-07T22:11:12.853719Z",
          "shell.execute_reply": "2022-09-07T22:11:12.852897Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "a68ca40a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f523733a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:12.857455Z",
          "iopub.status.busy": "2022-09-07T22:11:12.857093Z",
          "iopub.status.idle": "2022-09-07T22:11:12.862739Z",
          "shell.execute_reply": "2022-09-07T22:11:12.862003Z"
        },
        "origin_pos": 4,
        "tab": [
          "pytorch"
        ],
        "id": "f523733a"
      },
      "outputs": [],
      "source": [
        "def corr2d(X, K): \n",
        "    h, w = K.shape\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum() # producto de Haddamar\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "291ba298",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:12.865758Z",
          "iopub.status.busy": "2022-09-07T22:11:12.865490Z",
          "iopub.status.idle": "2022-09-07T22:11:12.893859Z",
          "shell.execute_reply": "2022-09-07T22:11:12.893159Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "291ba298",
        "outputId": "77ed4f7e-a9d7-4874-f6f6-1e7a155d0ddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19., 25.],\n",
              "        [37., 43.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "corr2d(X, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e924e935",
      "metadata": {
        "origin_pos": 8,
        "id": "e924e935"
      },
      "source": [
        "## Capas convolucionales \n",
        "\n",
        "Una capa convolucional aplica la función anterior y un sesgo escalar para producir una salida. Los dos parámetros de una capa convolucional\n",
        "son el núcleo y el sesgo escalar. Al entrenar modelos basados en capas convolucionales, normalmente inicializamos los núcleos al azar, tal como lo haríamos con una capa completamente conectada.\n",
        "\n",
        "Ahora estamos listos para implementar una capa convolucional bidimensional basado en la función `corr2d` definida anteriormente. En el método constructor `__init__`, declaramos `weight` y `bias` como los dos parámetros del modelo. La función de propagación directa\n",
        "llama a la función `corr2d` y agrega el sesgo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ab60c5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:12.897070Z",
          "iopub.status.busy": "2022-09-07T22:11:12.896800Z",
          "iopub.status.idle": "2022-09-07T22:11:12.901667Z",
          "shell.execute_reply": "2022-09-07T22:11:12.900953Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "92ab60c5"
      },
      "outputs": [],
      "source": [
        "class Conv2D(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return corr2d(x, self.weight) + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98db94a",
      "metadata": {
        "origin_pos": 12,
        "id": "b98db94a"
      },
      "source": [
        "## Primer ejemplo: Detector de bordes verticales.\n",
        "\n",
        "Tomemos un momento para analizar una aplicación simple de una capa convolucional:\n",
        "detectar el borde de un objeto en una imagen. Para ellos buscamos la ubicación del cambio de píxel.\n",
        "Primero, construimos una \"imagen\" de $6\\times 8$ píxeles.\n",
        "Las cuatro columnas del medio son negras (0) y el resto son blancas (1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce49cbf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:12.905046Z",
          "iopub.status.busy": "2022-09-07T22:11:12.904778Z",
          "iopub.status.idle": "2022-09-07T22:11:12.911147Z",
          "shell.execute_reply": "2022-09-07T22:11:12.910405Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "8ce49cbf",
        "outputId": "5ecda077-47b2-4d8a-a7b6-5935ec449592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "X = torch.ones((6, 8))\n",
        "X[:, 2:6] = 0\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f54073e",
      "metadata": {
        "origin_pos": 15,
        "id": "7f54073e"
      },
      "source": [
        "A continuación, construimos un núcleo `K` con una altura de 1 y un ancho de 2. Cuando realizamos al aplicar la convolución,\n",
        "si los elementos adyacentes horizontalmente son iguales,\n",
        "la salida es 0. De lo contrario, la salida es distinta de cero.\n",
        "Tenga en cuenta que este kernel es un caso especial de un operador de diferencias finitas. En la ubicación $(i,j)$ calcula $x_{i,j} - x_{(i+1),j}$, es decir, calcula la diferencia entre los valores de los píxeles adyacentes horizontalmente. Esta es una aproximación discreta de la primera derivada en la dirección horizontal. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93356622",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:12.914123Z",
          "iopub.status.busy": "2022-09-07T22:11:12.913856Z",
          "iopub.status.idle": "2022-09-07T22:11:12.917915Z",
          "shell.execute_reply": "2022-09-07T22:11:12.917164Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "93356622"
      },
      "outputs": [],
      "source": [
        "K = torch.tensor([[1.0, -1.0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21b1c84f",
      "metadata": {
        "origin_pos": 17,
        "id": "21b1c84f"
      },
      "source": [
        "Estamos listos para realizar una convolución con argumentos `X` (nuestra entrada) y `K` (nuestro kernel). Como puede ver, detectamos 1 para el borde de blanco a negro y -1 para el borde de negro a blanco. Todas las demás salidas toman valor 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ca4ec5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:12.921092Z",
          "iopub.status.busy": "2022-09-07T22:11:12.920661Z",
          "iopub.status.idle": "2022-09-07T22:11:12.927165Z",
          "shell.execute_reply": "2022-09-07T22:11:12.926439Z"
        },
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "04ca4ec5",
        "outputId": "c80b08ca-ebb7-4663-d4a3-e84b9f918389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "Y = corr2d(X, K)\n",
        "Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60adae61",
      "metadata": {
        "origin_pos": 19,
        "id": "60adae61"
      },
      "source": [
        "Al aplicar el kernel transpuesto obtenemos ceros. La versión **transpuesta** del kernel detecta bordes verticales y nuestra imagen no los tiene\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.t()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-B2JMg3PGZe",
        "outputId": "dd016b63-54dc-4133-ebe0-7576683daa77"
      },
      "id": "h-B2JMg3PGZe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987bc7fa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:12.930716Z",
          "iopub.status.busy": "2022-09-07T22:11:12.929982Z",
          "iopub.status.idle": "2022-09-07T22:11:12.937612Z",
          "shell.execute_reply": "2022-09-07T22:11:12.936667Z"
        },
        "origin_pos": 20,
        "tab": [
          "pytorch"
        ],
        "id": "987bc7fa",
        "outputId": "8ec56e01-2418-403e-ecc8-8b1dd601284c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "corr2d(X.t(), K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c0cb59",
      "metadata": {
        "origin_pos": 21,
        "id": "d4c0cb59"
      },
      "source": [
        "## Aprendiendo un kernel\n",
        "\n",
        "Diseñar un detector de bordes por diferencias finitas `[1, -1]` puede ser útil si sabemos que esto es precisamente lo que estamos buscando. Sin embargo, a medida que observamos núcleos más grandes, y considerar capas sucesivas de convoluciones, puede ser imposible especificar\n",
        "precisamente lo que cada filtro debe hacer manualmente.\n",
        "\n",
        "Ahora veamos si podemos aprender el núcleo que generó `Y` a partir de `X` mirando solo los pares de entrada-salida. Primero construimos una capa convolucional e inicializa su kernel como un tensor aleatorio. A continuación, en cada iteración, usaremos el error al cuadrado para comparar `Y` con la salida de la capa convolucional. Luego podemos calcular el gradiente para actualizar el kernel. Por el bien de la simplicidad,\n",
        "en el siguiente usamos la clase incorporada para capas convolucionales bidimensionales e ignorar el sesgo. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "088cb348",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:12.941177Z",
          "iopub.status.busy": "2022-09-07T22:11:12.940576Z",
          "iopub.status.idle": "2022-09-07T22:11:12.953566Z",
          "shell.execute_reply": "2022-09-07T22:11:12.952831Z"
        },
        "origin_pos": 23,
        "tab": [
          "pytorch"
        ],
        "id": "088cb348",
        "outputId": "69adabed-4e76-449d-9d1a-15f12d80ca4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, loss 14.618\n",
            "epoch 4, loss 3.986\n",
            "epoch 6, loss 1.297\n",
            "epoch 8, loss 0.475\n",
            "epoch 10, loss 0.185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "# Constructor de la clase convolucion bidimensional de torch.\n",
        "# Hemos apagado a los bias. El kernel es de la forma (1,2)\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n",
        "\n",
        "# La capa convolucional de dos dimensiones usa entrada  y salidas de cuatro \n",
        "# dimensiones y en el formato de (ejemplo, canal, alto, ancho). Para este \n",
        "# ejemplo el tamaño de lote (número de ejemplos en el lote) \n",
        "# y el número de canales son ambos 1\n",
        "\n",
        "X = X.reshape((1, 1, 6, 8))\n",
        "Y = Y.reshape((1, 1, 6, 7))\n",
        "lr = 3e-2  # Learning rate\n",
        "\n",
        "for i in range(10):\n",
        "    Y_hat = conv2d(X)\n",
        "    l = (Y_hat - Y) ** 2 ## minimos cuadrados\n",
        "    conv2d.zero_grad()\n",
        "    l.sum().backward()\n",
        "    # actualizamos los pesos\n",
        "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
        "    if (i + 1) % 2 == 0:\n",
        "        print(f'epoch {i + 1}, loss {l.sum():.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b655589",
      "metadata": {
        "origin_pos": 25,
        "id": "5b655589"
      },
      "source": [
        "Tras 10 iteraciones tenemos una buena aproximación al kernel esperado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a20b979",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:12.956586Z",
          "iopub.status.busy": "2022-09-07T22:11:12.956318Z",
          "iopub.status.idle": "2022-09-07T22:11:12.962159Z",
          "shell.execute_reply": "2022-09-07T22:11:12.961455Z"
        },
        "origin_pos": 27,
        "tab": [
          "pytorch"
        ],
        "id": "5a20b979",
        "outputId": "704c3daa-5e98-4bb9-d6cb-afcd2a7c984e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9414, -1.0287]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "conv2d.weight.data.reshape((1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ade113a2",
      "metadata": {
        "origin_pos": 29,
        "id": "ade113a2"
      },
      "source": [
        "## Correlación cruzada y convoluciones.\n",
        "\n",
        "\n",
        "Hasta aquí hemos hablado de convoluciones, pero en realidad hay una diferencia entre nuestra operación y una convolución propiamente dicha. La operación que realizamos es en realidad una correlación cruzada. ¿Qué pasaría si estas capas realizaran operaciones de convolución estrictas en lugar de correlaciones cruzadas? Para obtener el resultado de la operación de *convolución* estricta, solo necesitamos voltear el tensor kernel bidimensional tanto horizontal como verticalmente, y luego realizar la operación de *correlación cruzada* con el tensor de entrada.\n",
        "\n",
        "Cabe señalar que, dado que los núcleos se aprenden a partir de datos en el aprendizaje, las salidas de las capas convolucionales no se ven afectadas no importa si llevamos a cabo convoluciones estrictas o las operaciones de correlación cruzada.\n",
        "\n",
        "De acuerdo con la terminología estándar con la literatura de aprendizaje profundo, seguiremos refiriéndonos a la operación de correlación cruzada\n",
        "como una convolución aunque, estrictamente hablando, es ligeramente diferente. Además, usamos el término *elemento* para referirnos a una entrada (o componente) de cualquier tensor que represente una representación de capa o un núcleo de convolución.\n",
        "\n",
        "\n",
        "## Mapa de características y Campo receptivo.\n",
        "\n",
        "La salida de la capa convolucional a veces se llama un **mapa de características**, pues son representaciones aprendidas (características) en las dimensiones espaciales (por ejemplo, ancho y alto) que se alimentan a la capa subsiguiente. En las CNN, para cualquier elemento $x$ de alguna capa, su **campo receptivo** se refiere a todos los elementos (de todas las capas anteriores) que puede afectar el cálculo de $x$ durante la propagación directa. Tenga en cuenta que el campo receptivo puede ser mayor que el tamaño real de la entrada. \n",
        "\n",
        "Sigamos usando la figura anterior para explicar el campo receptivo.\n",
        "\n",
        "![](http://d2l.ai/_images/correlation.svg)\n",
        "\n",
        "Dado el kernel o núcleo de convolución $2 \\times 2$, el campo receptivo del elemento de salida sombreado (de valor $19$) es los cuatro elementos en la parte sombreada de la entrada. Ahora vamos a denotar la salida de forma $2 \\times 2$ como $\\mathbf{Y}$ y considere una CNN más profunda con una capa convolucional adicional de $2 \\times 2$ que toma $\\mathbf{Y}$ como su entrada, salida un solo elemento $z$. En este caso, el campo receptivo de $z$ en $\\mathbf{Y}$ incluye los cuatro elementos de $\\mathbf{Y}$, mientras que el campo receptivo en la entrada original incluye los nueve elementos de entrada. De este modo, cuando un elemento en un mapa de características necesita un campo receptivo más grande podemos construir una red más profunda.\n",
        "\n",
        "Los campos receptivos derivan su nombre de la neurofisiología. En una serie de experimentos en una variedad de animales y diferentes estímulos, Hubel y Wiesel exploraron la respuesta de la corteza visual sobre dichos estímulos. En general encontraron que los niveles inferiores responden a los bordes y  formas relacionadas. Posteriormente, se ilustró este efecto sobre la naturaleza imágenes con, lo que solo se puede llamar, núcleos convolucionales.\n",
        "\n",
        "\n",
        "Resulta que esta relación incluso es válida para las características calculadas por capas más profundas de redes entrenadas en tareas de clasificación de imágenes.  Baste decir que las circunvoluciones han demostrado ser una herramienta increíblemente poderosa para la visión por computadora, tanto en biología como en código. Como tal, no sorprende (en retrospectiva) que anunciaran el éxito reciente en el aprendizaje profundo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 1,
        "id": "33e0bf06"
      },
      "source": [
        "# Padding y Stride\n",
        "\n",
        "Volvamos a la figura anterior.\n",
        "\n",
        "![](http://d2l.ai/_images/correlation.svg)\n",
        "\n",
        "La entrada tenía una altura y un ancho de 3 y el núcleo de convolución tenía una altura y un ancho de 2, produciendo una representación de salida con dimensión $2\\times2$. Dijimos que para una entrada  la forma $n_h\\times n_w$ y kernel de la forma $k_h\\times k_w$, la forma de salida será $(n_h-k_h+1) \\times (n_w-k_w+1)$. Solo podemos mover el kernel hasta que se agote los píxeles a los que aplicar la convolución.\n",
        "\n",
        "A continuación exploraremos una serie de técnicas, incluyendo relleno y convoluciones con strides o trancos, que ofrecen más control sobre el tamaño de la salida. Como motivación, tenga en cuenta que dado que los núcleos generalmente tienen ancho y alto mayor que $1$, después de aplicar muchas convoluciones sucesivas, tendemos a terminar con resultados que son\n",
        "considerablemente más pequeño que nuestra entrada. Si comenzamos con una imagen de $240 \\times 240$ píxeles, tras $10$ capas de $5 \\times 5$ convoluciones, la imágen se recude a $200 \\times 200$ píxeles, cortando $30 \\%$ de la imagen y con ella borrando cualquier información interesante\n",
        "en los límites de la imagen original. **Padding** o agregar relleno es la herramienta más popular para manejar este problema. En otros casos, podemos querer reducir la dimensionalidad drásticamente, por ejemplo, si encontramos que la resolución de entrada original es difícil de manejar. Las **convoluciones con strides o trancos** son una técnica popular que puede ayudar en estos casos.\n",
        "\n",
        "## Padding\n",
        "\n",
        "Como se describió anteriormente, un problema al aplicar capas convolucionales es que tendemos a perder píxeles en el perímetro de nuestra imagen. Considere la próxima imagen que representa la utilización de píxeles como una función del tamaño del kernel de convolución y la posición dentro de la imagen. Los píxeles de las esquinas apenas se utilizan.\n",
        "\n",
        "![](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-reuse.svg?raw=1)\n",
        "\n",
        "Dado que normalmente usamos núcleos pequeños, para cualquier convolución dada, es posible que solo perdamos unos pocos píxeles. Pero el número de píxeles crece a medida que aplicamos muchas capas convolucionales sucesivas Una solución directa a este problema es agregar píxeles adicionales de relleno alrededor del límite de nuestra imagen de entrada, aumentando así el tamaño efectivo de la imagen. Por lo general, establecemos los valores de los píxeles adicionales en cero. En la siguiente imágen rellenamos una entrada de $3 \\times 3$, aumentando su tamaño a $5 \\times 5$. La salida correspondiente luego aumenta a una matriz de $4 \\times 4$. Las partes sombreadas son el primer elemento de salida, así como los elementos de entrada y de kernel utilizados para el cálculo de salida:\n",
        "\n",
        "$$0\\times0+0\\times1+0\\times2+0\\times3=0$$.\n",
        "\n",
        "![](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-pad.svg?raw=1)\n",
        "\n",
        "\n",
        "En general, si agregamos $p_h$ filas de relleno  (mitad arriba, mitad abajo)\n",
        " y $p_w$ columnas de relleno (mitad a la izqueirda, mitad a al derecha),\n",
        "la salida será:\n",
        "\n",
        "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$\n",
        "\n",
        "Esto significa que la altura y el ancho de la salida aumentará en $p_h$ y $p_w$, respectivamente.\n",
        "\n",
        "En muchos casos, querremos configurar $p_h=k_h-1$ y $p_w=k_w-1$ para dar a la entrada y salida la misma altura y anchura. Esto facilitará la predicción de la forma de salida de cada capa al construir la red. Asumiendo que $k_h$ es impar aquí, rellenaremos $p_h/2$ filas en ambos lados de la altura. Si $k_h$ es par, una posibilidad es pad $\\lceil p_h/2\\rceil$ filas en la parte superior de la entrada y $\\lfloor p_h/2\\rfloor$ filas en la parte inferior. Rellenaremos ambos lados del ancho de la misma manera.\n",
        "\n",
        "Las CNN suelen utilizar núcleos de convolución con valores impares de alto y ancho, como 1, 3, 5 o 7. Elegir tamaños de kernel impares tiene la ventaja\n",
        "que podemos preservar la dimensionalidad mientras rellena con el mismo número de filas en la parte superior e inferior, y el mismo número de columnas a izquierda y derecha.\n",
        "\n",
        "Además, esta práctica de usar núcleos impares y padding para preservar con precisión la dimensionalidad ofrece un beneficio adicional. Para cualquier tensor bidimensional `X`, cuando el tamaño del núcleo es impar: *\n",
        "  * el número de filas y columnas de relleno por los bordes son iguales\n",
        "  * la salida tiene la misma forma entrada\n",
        "  * el campo receptivo de la salida `Y[i, j]` se calcula con una ventana centrada en `X[i, j]` a partir del kernel\n",
        "\n",
        "En el siguiente ejemplo, creamos una capa convolucional bidimensional\n",
        "con un kernel de altura y ancho igual a 3. Además agregamos 1 píxel de relleno en todos los bordes. Dada una entrada con una altura y un ancho de 8, encontramos que la altura y el ancho de la salida también es 8.\n"
      ],
      "id": "33e0bf06"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:10:12.386361Z",
          "iopub.status.busy": "2022-09-07T22:10:12.385710Z",
          "iopub.status.idle": "2022-09-07T22:10:13.272409Z",
          "shell.execute_reply": "2022-09-07T22:10:13.271491Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "48cff272",
        "outputId": "29377f8f-7f78-4fa1-9818-c05ff89e683d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "# Definimos una función auxiliar para calcular convoluciones. Se inicializan\n",
        "# los pesos de la capa convolucional y adapta la dimensionalidad segun \n",
        "# corresponde\n",
        "def comp_conv2d(conv2d, X):\n",
        "    # (1, 1) es el tamaño de mini lote y el número de canal, en este ejemplo 1\n",
        "    X = X.reshape((1, 1) + X.shape)\n",
        "    Y = conv2d(X)\n",
        "    # eliminamos los dos primeros indices\n",
        "    return Y.reshape(Y.shape[2:])\n",
        "# padding igal a 1 en cada borde, es decir por 2 fila y 2 por columna\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\n",
        "X = torch.rand(size=(8, 8))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "id": "48cff272"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 5,
        "id": "4b492a8a"
      },
      "source": [
        "Ahora podemos hacer el mismo truco para cualquier kernel de tamaño impar"
      ],
      "id": "4b492a8a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:10:13.276964Z",
          "iopub.status.busy": "2022-09-07T22:10:13.276436Z",
          "iopub.status.idle": "2022-09-07T22:10:13.283422Z",
          "shell.execute_reply": "2022-09-07T22:10:13.282648Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "e9966560",
        "outputId": "863686ba-15b9-4bbc-da70-accf820b13d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Para un kernel con altura igual 5 y ancho igual a 3, el padding debe ser\n",
        "# 2 filas arriba, 2 filas abajo, una columnas a la izquiera y una columna a la \n",
        "# derecha\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=(7, 5), padding=(3, 2))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "id": "e9966560"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 9,
        "id": "d2229d43"
      },
      "source": [
        "## Stride o trancos\n",
        "\n",
        "Al calcular convoluciones, empezamos con la ventana de convolución en la esquina superior izquierda del tensor de entrada, y luego deslizamos sobre todas las ubicaciones tanto hacia abajo como hacia la derecha. En los ejemplos anteriores, por defecto deslizamos un elemento a la vez. Sin embargo, a veces, ya sea por eficiencia computacional o porque deseamos reducir la muestra, movemos nuestra ventana más de un elemento a la vez, saltándose las ubicaciones intermedias. Esto es particularmente útil si la el kernel de la convolución es grande ya que captura una gran área de la imagen subyacente.\n",
        "\n",
        "Nos referimos al número de filas y columnas atravesadas por calculo como *stride* o tranco. Hasta ahora, hemos usado trancos de 1, tanto para la altura como para el ancho. A veces, podemos querer usar un paso más grande.\n",
        "En la siguiente figura se muestra un ejemplo con un tranco 3 en vertical y 2 en horizontal. Las partes sombreadas son los elementos de salida, así como los elementos del kernel y de la entrada utilizados para el cálculo de salida: \n",
        "\n",
        "$$0\\times0+0\\times1+1\\times2+2\\times3=8$$\n",
        "$$0\\times0+6\\times1+0 \\times2+0\\times3=6$$\n",
        "\n",
        "Podemos ver que cuando se genera el segundo elemento de la primera columna,\n",
        "la ventana de convolución se desliza hacia abajo tres filas. La ventana de convolución desliza dos columnas a la derecha cuando se genera el segundo elemento de la primera fila. Cuando la ventana de convolución continúa deslizando dos columnas hacia la derecha en la entrada, no hay salida porque el elemento de entrada no puede llenar la ventana (a menos que agreguemos otra columna de relleno).\n",
        "\n",
        "![](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-stride.svg?raw=1)\n",
        "\n",
        "En general, para un tranco de alto $s_h$ y de ancho $s_w$ la salida es de la forma \n",
        "\n",
        "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$$\n",
        "\n",
        "Fijando $p_h=k_h-1$ y $p_w=k_w-1$, la salida se reduce a \n",
        "$\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$.\n",
        "Si el ancho y el alto de entrada es divisible por el ancho y el alto del tranco encontes la expresión se reduce aun más a $(n_h/s_h) \\times (n_w/s_w)$.\n",
        "\n",
        "Por ejemplo, podemos fijar el ancho y el alto de nuestro tranco en 1\n"
      ],
      "id": "d2229d43"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:10:13.287287Z",
          "iopub.status.busy": "2022-09-07T22:10:13.286625Z",
          "iopub.status.idle": "2022-09-07T22:10:13.294572Z",
          "shell.execute_reply": "2022-09-07T22:10:13.293671Z"
        },
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "id": "99a3dbe0",
        "outputId": "17747e3a-4486-4362-9dd1-12884826da01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "id": "99a3dbe0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 13,
        "id": "ddb00755"
      },
      "source": [
        "O hace cosas más complejas.\n",
        "\n",
        "\n"
      ],
      "id": "ddb00755"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:10:13.298199Z",
          "iopub.status.busy": "2022-09-07T22:10:13.297668Z",
          "iopub.status.idle": "2022-09-07T22:10:13.304538Z",
          "shell.execute_reply": "2022-09-07T22:10:13.303799Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "ad5977ab",
        "outputId": "f7e8f5b1-65ed-42d9-a110-94a01c2d043a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "id": "ad5977ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 1,
        "id": "7cba414b"
      },
      "source": [
        "# Entradas y salidas multicanal\n",
        "\n",
        "Hasta aquí hemos ignorado completamente la naturaleza tridimensional de las imágenes. Las imágenes en color tradicionales tienen los canales RGB para indicar la intensidad de rojo, verde y azul. Por esto tenemos 3 indices: ancho, alto y canal. Hasta ahora, hemos simplificado todos nuestros ejemplos numéricos trabajando con un solo canal a la entrada y un solo canal de salida. Esto nos permitió pensar en nuestras entradas, núcleos de convolución, y generar cada uno como tensores bidimensionales.\n",
        "\n",
        "Cuando agregamos canales a la mezcla, nuestras entradas y representaciones ocultas ambos se convierten en tensores tridimensionales. Por ejemplo, cada imagen de entrada RGB tiene la forma $3\\times h\\times w$. Nos referimos a este eje, con un tamaño de 3, como la dimensión *canal*. La noción de\n",
        "canales es tan antiguo como las propias CNN.\n",
        "En esta sección, vamos a echar un vistazo más profundo a las convoluciones con múltiples canales de entrada y salida.\n",
        "\n",
        "## Entrada multicanal\n",
        "\n",
        "Cuando los datos de entrada contienen múltiples canales, necesitamos construir una convolución con el mismo número de canales que los datos de entrada, para que pueda realizar una convolución con los datos de entrada. Suponiendo que el número de canales para los datos de entrada es $c_i$, el número de canales de entrada del kernel de convolución también debe ser $c_i$. Si la forma de la ventana de nuestro kernel de convolución es $k_h\\times k_w$, entonces cuando $c_i=1$, podemos pensar en nuestro kernel de convolución como un tensor bidimensional de forma $k_h\\times k_w$.\n",
        "\n",
        "Sin embargo, cuando $c_i>1$, necesitamos un kernel que contenga un tensor de forma $k_h\\times k_w$ para *cada* canal de entrada. Concatenando estos tensores $c_i$ juntos produce un kernel de convolución de forma $c_i\\times k_h\\times k_w$. Dado que el tensor de entrada y el de convolución tienen canales $c_i$, podemos realizar una convolución\n",
        "en el tensor bidimensional de la entrada y el tensor bidimensional del kernel de convolución para cada canal, sumando los resultados de $c_i$ juntos (suma sobre los canales) para producir un tensor bidimensional.\n",
        "Este es el resultado de una convolución bidimensional\n",
        "entre una entrada multicanal y un núcleo de convolución de múltiples canales de entrada. \n",
        "\n",
        "En la siguiente figura hay un ejemplo de una convolucion bidimensional con dos canales de entrada. Las partes sombreadas son el primer elemento de salida. así como los elementos de tensor de kernel y de entrada utilizados para el cálculo de salida:\n",
        "\n",
        "$$(1\\times1+2\\times2+4\\times3+5\\times4)+(0\\times0+1\\times1+3\\times2+4\\times3)=56$$\n",
        "\n",
        "![](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-multi-in.svg?raw=1)\n",
        "\n",
        "Tratemos de implementar un ejemplo de esto. Recordemos que solo es una convolución por cada canal y luego sumamos por cada canal"
      ],
      "id": "7cba414b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:07:50.892804Z",
          "iopub.status.busy": "2022-09-07T22:07:50.892114Z",
          "iopub.status.idle": "2022-09-07T22:07:52.836715Z",
          "shell.execute_reply": "2022-09-07T22:07:52.835886Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "56313b3b"
      },
      "outputs": [],
      "source": [
        "import torch\n"
      ],
      "id": "56313b3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:07:52.842257Z",
          "iopub.status.busy": "2022-09-07T22:07:52.841683Z",
          "iopub.status.idle": "2022-09-07T22:07:52.846057Z",
          "shell.execute_reply": "2022-09-07T22:07:52.845340Z"
        },
        "origin_pos": 4,
        "tab": [
          "pytorch"
        ],
        "id": "1ec53857"
      },
      "outputs": [],
      "source": [
        "def corr2d_multi_in(X, K):\n",
        "    # Iteramos por cada canal y sumamos\n",
        "    return sum(corr2d(x, k) for x, k in zip(X, K))"
      ],
      "id": "1ec53857"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 6,
        "id": "818640b2"
      },
      "source": [
        "Tratemos de reproducir los resultados de la figura para validar que la función actúe correctamente.\n"
      ],
      "id": "818640b2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:07:52.850692Z",
          "iopub.status.busy": "2022-09-07T22:07:52.850003Z",
          "iopub.status.idle": "2022-09-07T22:07:52.900474Z",
          "shell.execute_reply": "2022-09-07T22:07:52.898534Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "81c2b999",
        "outputId": "70e530cd-709a-4059-9082-b86073a82e42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  72.],\n",
              "        [104., 120.]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
        "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
        "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
        "\n",
        "corr2d_multi_in(X, K)"
      ],
      "id": "81c2b999"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "0b4e7fae"
      },
      "source": [
        "## Salidas con múltiples canales\n",
        "\n",
        "Independientemente del número de canales de entrada, hasta ahora siempre terminamos con un canal de salida. Sin embargo, resulta esencial tener múltiples canales en cada capa. En las arquitecturas de redes neuronales más populares, en realidad aumentamos la dimensión del canal a medida que profundizamos en la red neuronal. Típicamente reducimos el tamaño de la imagen y para compensar la perdida de resolución espacial aumentamos la *profundidad de canal*. Intuitivamente, se puede pensar en cada canal como respondiendo a un conjunto diferente de características. La realidad es un poco más complicada que esto. Una interpretación ingenua sugeriría que las representaciones se aprenden de forma independiente por píxel o por canal. En cambio, los canales están optimizados para ser útiles en conjunto. Esto significa que en lugar de asignar un solo canal a un detector de borde,puede significar simplemente que alguna dirección en el espacio del canal corresponde a la detección de bordes.\n",
        "\n",
        "Denotamos por $c_i$ y $c_o$ el número de canales de entrada y salida, respectivamente, y $k_h$ y $k_w$ la altura y el ancho del kernel.\n",
        "Para obtener una salida con múltiples canales, podemos crear un tensor kernel de forma $c_i\\times k_h\\times k_w$ para *cada* canal de salida.\n",
        "Los concatenamos en la dimensión del canal de salida, de modo que la forma del núcleo de convolución es $c_o\\times c_i\\times k_h\\times k_w$. En las operaciones de convolución, se calcula el resultado en cada canal de salida\n",
        "del kernel de convolución correspondiente a ese canal de salida\n",
        "y toma entrada de todos los canales en el tensor de entrada.\n",
        "\n",
        "Implementamos una función de convolución\n",
        "para calcular la salida de múltiples canales como se muestra a continuación."
      ],
      "id": "0b4e7fae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:07:52.922621Z",
          "iopub.status.busy": "2022-09-07T22:07:52.911852Z",
          "iopub.status.idle": "2022-09-07T22:07:52.937237Z",
          "shell.execute_reply": "2022-09-07T22:07:52.936039Z"
        },
        "origin_pos": 9,
        "tab": [
          "pytorch"
        ],
        "id": "464a8703"
      },
      "outputs": [],
      "source": [
        "def corr2d_multi_in_out(X, K):\n",
        "    # Iteramos en la dirección 0 de `K` aplicamos convolución multicanal\n",
        "    # al mismo tensor `X`, luego los apilamos\n",
        "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
      ],
      "id": "464a8703"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 10,
        "id": "a30ab32b"
      },
      "source": [
        "Podemos armar un ejemplo de un kernel con muchos canales de salida por ejemplo como mostramos en el siguiente código "
      ],
      "id": "a30ab32b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:07:52.942345Z",
          "iopub.status.busy": "2022-09-07T22:07:52.941533Z",
          "iopub.status.idle": "2022-09-07T22:07:52.950301Z",
          "shell.execute_reply": "2022-09-07T22:07:52.949237Z"
        },
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "id": "9c838373",
        "outputId": "34a46084-9f6d-45e1-a972-74dbfd3d265c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "K = torch.stack((K, K + 1, K + 2), 0)\n",
        "K.shape"
      ],
      "id": "9c838373"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "c5ba9489"
      },
      "source": [
        "Ahora aplicamos la función que recien armamos al tensor recien creado. Ahora la salida tiene 3 canales. Podemos ver, en este caso que el primer canal de salida coincide con lo calculado anteriormente. "
      ],
      "id": "c5ba9489"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:07:52.959281Z",
          "iopub.status.busy": "2022-09-07T22:07:52.956421Z",
          "iopub.status.idle": "2022-09-07T22:07:52.977904Z",
          "shell.execute_reply": "2022-09-07T22:07:52.976290Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "fe123dde",
        "outputId": "5a1a9e5b-bf9a-4900-a31f-2c67b03ab436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 56.,  72.],\n",
              "         [104., 120.]],\n",
              "\n",
              "        [[ 76., 100.],\n",
              "         [148., 172.]],\n",
              "\n",
              "        [[ 96., 128.],\n",
              "         [192., 224.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "corr2d_multi_in_out(X, K)"
      ],
      "id": "fe123dde"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 14,
        "id": "91672450"
      },
      "source": [
        "## Convoluciones... ¿de tamaño $1\\times 1$?\n",
        "\n",
        "Al principio, una convolución $1 \\times 1$, es decir, $k_h = k_w = 1$, \n",
        "no parece tener mucho sentido. Después de todo, una convolución analiza píxeles adyacentes. Una convolución de $1 \\times 1$ obviamente no lo hace. No obstante, son operaciones populares que a veces se incluyen\n",
        "en los diseños de redes profundas complejas. Veamos con cierto detalle lo que realmente hace.\n",
        "\n",
        "Debido a que se utiliza la ventana mínima, la convolución $1\\times 1$ pierde la capacidad de capas convolucionales más grandes reconocer patrones que consisten en interacciones entre elementos adyacentes en las dimensiones de alto y ancho. El único cálculo de la convolución $1\\times 1$ ocurre en la dimensión del canal.\n",
        "\n",
        "La siguiente figura muestra el cálculo de una convolución usando el núcleo de convolución $1\\times 1$ con 3 canales de entrada y 2 canales de salida. Las entradas y salidas tienen la misma altura y anchura. Cada elemento de la salida se deriva de una combinación lineal de elementos *en la misma posición* en la imagen de entrada. La capa convolucional $1\\times 1$ podría pensarse como una capa densa aplicada en cada ubicación de píxel \n",
        "para transformar los valores de entrada correspondientes de $c_i$ en valores de salida de $c_o$. Debido a que esta sigue siendo una capa convolucional, los pesos están vinculados a través de la ubicación del píxel. Por lo tanto, la capa convolucional $1\\times 1$ requiere pesos $c_o\\times c_i$ (más el sesgo). Las capas convolucionales, como toda capa, es continuada por una función de activación. Esto asegura que las convoluciones de $1 \\times 1$ no pueden ser imitadas por convoluciones.\n",
        "\n",
        "![](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-1x1.svg?raw=1)\n",
        "\n",
        "\n",
        "Veamos como esto se implementa en la práctica. Consideremos una capa densa aplicada a una convolución $1 \\times 1$, Slo necesitamos adaptar la entrada ya la salida antes de la multiplicacion de matrices.\n"
      ],
      "id": "91672450"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:07:52.986807Z",
          "iopub.status.busy": "2022-09-07T22:07:52.983017Z",
          "iopub.status.idle": "2022-09-07T22:07:52.991763Z",
          "shell.execute_reply": "2022-09-07T22:07:52.990625Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "d77c1d16"
      },
      "outputs": [],
      "source": [
        "def corr2d_multi_in_out_1x1(X, K):\n",
        "    c_i, h, w = X.shape\n",
        "    c_o = K.shape[0]\n",
        "    X = X.reshape((c_i, h * w))\n",
        "    K = K.reshape((c_o, c_i))\n",
        "    # multiplicación de matrices como en una capa densa.\n",
        "    Y = torch.matmul(K, X)\n",
        "    return Y.reshape((c_o, h, w))"
      ],
      "id": "d77c1d16"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 16,
        "id": "0f42af95"
      },
      "source": [
        "El resultado es indistinguible de lo descripto hasta aquí. De hecho podemos hacer la compración. \n"
      ],
      "id": "0f42af95"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:07:52.997200Z",
          "iopub.status.busy": "2022-09-07T22:07:52.996506Z",
          "iopub.status.idle": "2022-09-07T22:07:53.001368Z",
          "shell.execute_reply": "2022-09-07T22:07:53.000536Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "902204f4"
      },
      "outputs": [],
      "source": [
        "X = torch.normal(0, 1, (3, 3, 3))\n",
        "K = torch.normal(0, 1, (2, 3, 1, 1))"
      ],
      "id": "902204f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:07:53.005696Z",
          "iopub.status.busy": "2022-09-07T22:07:53.005159Z",
          "iopub.status.idle": "2022-09-07T22:07:53.011793Z",
          "shell.execute_reply": "2022-09-07T22:07:53.010916Z"
        },
        "origin_pos": 19,
        "tab": [
          "pytorch"
        ],
        "id": "98082c11"
      },
      "outputs": [],
      "source": [
        "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
        "Y2 = corr2d_multi_in_out(X, K)\n",
        "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
      ],
      "id": "98082c11"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 1,
        "id": "d131b356"
      },
      "source": [
        "# Pooling\n",
        "\n",
        "En muchos casos, nuestra tarea final plantea alguna pregunta global sobre la imagen, por ejemplo, *¿contiene un gato?*. En consecuencia, las unidades de nuestra capa final debe ser sensible a toda la entrada. Agregando información gradualmente, produciendo campos receptivos cada vez más grandes, logramos este objetivo de finalmente aprender una representación global, manteniendo todas las ventajas de las capas convolucionales en las capas intermedias de procesamiento. Cuanto más nos adentramos en la red, cuanto mayor sea el campo receptivo (en relación con la entrada) a la que cada nodo oculto es sensible. Reducir la resolución espacial acelera este proceso, ya que los núcleos de convolución cubren un área efectiva más grande.\n",
        "\n",
        "Además, al detectar características de nivel inferior, como bordes a menudo queremos que nuestras representaciones sean algo invariables a la translación. Por ejemplo, si tomamos la imagen `X` con una delimitación nítida entre blanco y negro y desplazamos toda la imagen un píxel a la derecha, es decir, `Z[i, j] = X[i, j + 1]`, entonces la salida para la nueva imagen `Z` podría ser muy diferente. El borde se habrá desplazado un píxel. En realidad, los objetos casi nunca se encuentran exactamente en el mismo lugar. De hecho, incluso con un trípode y un objeto estacionario,\n",
        "vibración de la cámara debido al movimiento del obturador podría cambiar todo por un píxel más o menos (Las cámaras de gama alta están cargadas con características especiales para abordar este problema).\n",
        "\n",
        "Hablaremos ahora de *capas de pooling*, que sirven al doble propósito de\n",
        "mitigar la sensibilidad de las capas convolucionales a la ubicación y de representaciones de reducción de muestreo espacial.\n",
        "\n",
        "## Pooling con máximos y con promedios\n",
        "\n",
        "Al igual que las capas convolucionales, los operadores *pooling*\n",
        "consisten en una ventana de forma fija que se desliza sobre\n",
        "todas las regiones de la entrada con un stride y calcula una sola salida para cada ubicación atravesada por la ventana de forma fija (a veces conocida como la *ventana de pooling*). Sin embargo, a diferencia del cálculo de convoluciones, la capa de pooling no contiene parámetros (no hay *kernel*). En cambio, los operadores de pooling son deterministas, típicamente calculando el valor máximo o el promedio de los elementos en la ventana de pooling. Estas operaciones se denominan *pooling con máximo* (*max-pooling* para abreviar) y *pooling con promedio*, respectivamente.\n",
        "\n",
        "*El pooling con promedio* es esencialmente tan antigua como las CNN. La idea es similar a reducir la resolución de una imagen. En lugar de simplemente tomar el valor de cada segundo (o tercio) píxel para la imagen de menor resolución, podemos promediar los píxeles adyacentes para obtener\n",
        "una imagen con mejor relación señal/ruido ya que estamos combinando la información de múltiples píxeles adyacentes. *Max-pooling* se introdujo  en el contexto de la neurociencia cognitiva para describir cómo la agregación de información podría agregarse jerárquicamente para el propósito de reconocimiento de objetos, y una versión anterior en reconocimiento de voz. En casi todos los casos, max-pooling, es preferible.\n",
        "\n",
        "En ambos casos, al igual que con la convolución, podemos pensar en la ventana de pooling partiendo de la parte superior izquierda del tensor de entrada deslizándose por el tensor de entrada de izquierda a derecha y de arriba a abajo. En cada ubicación a la que llega la ventana de pooling, calcula el máximo o el promedio valor del subtensor de entrada en la ventana, dependiendo de si se emplea la pooling con máximo o con promedio.\n",
        "\n",
        "\n",
        "![](http://d2l.ai/_images/pooling.svg)\n",
        "\n",
        "El tensor de salida en la figura tiene tamaño $2\\times2$. Los elementos fueron calulados de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\\max(0, 1, 3, 4)=4,\\\\\n",
        "\\max(1, 2, 4, 5)=5,\\\\\n",
        "\\max(3, 4, 6, 7)=7,\\\\\n",
        "\\max(4, 5, 7, 8)=8.\\\\\n",
        "$$\n",
        "\n",
        "En términos más generales, podemos definir una capa de pooling $p \\times q$ sobre una región de dicho tamaño. Volviendo al problema de la detección de bordes, podemos usar la salida de la capa convolucional como entrada para $2\\times 2$ max-pooling. Denotaremos con `X` la entrada de la capa convolucional y `Y` la salida de la capa de pooling. Independientemente de si los valores de `X[i, j]`, `X[i, j + 1]`, `X[i+1, j]` y `X[i+1, j + 1]` son diferentes, la capa de pooling siempre genera `Y[i, j] = 1`. Es decir, usando la capa de pooling con máximo $2\\times 2$, podemos detectar si el patrón reconocido por la capa convolucional no mueve más de un elemento en altura o anchura.\n",
        "\n",
        "En el siguiente código, implementamos la propagación directa de la capa de pooling en la función `pool2d`. Esta función es similar a la función `corr2d`. Sin embargo, no se necesita kernel, calculando la salida como el máximo o el promedio de cada región en la entrada. "
      ],
      "id": "d131b356"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:05.742183Z",
          "iopub.status.busy": "2022-09-07T22:11:05.741557Z",
          "iopub.status.idle": "2022-09-07T22:11:07.584093Z",
          "shell.execute_reply": "2022-09-07T22:11:07.583234Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "7465296f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "id": "7465296f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:07.588217Z",
          "iopub.status.busy": "2022-09-07T22:11:07.587649Z",
          "iopub.status.idle": "2022-09-07T22:11:07.593654Z",
          "shell.execute_reply": "2022-09-07T22:11:07.592920Z"
        },
        "origin_pos": 4,
        "tab": [
          "pytorch"
        ],
        "id": "8625a0ce"
      },
      "outputs": [],
      "source": [
        "def pool2d(X, pool_size, mode='max'):\n",
        "    p_h, p_w = pool_size\n",
        "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            if mode == 'max':\n",
        "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
        "            elif mode == 'avg':\n",
        "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
        "    return Y"
      ],
      "id": "8625a0ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 6,
        "id": "4692c1fc"
      },
      "source": [
        "Veamos dos ejemplos para validar esta función."
      ],
      "id": "4692c1fc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:07.597000Z",
          "iopub.status.busy": "2022-09-07T22:11:07.596572Z",
          "iopub.status.idle": "2022-09-07T22:11:07.625908Z",
          "shell.execute_reply": "2022-09-07T22:11:07.625186Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "98aae7b4",
        "outputId": "a42fb91b-ba55-4e0e-b3c6-ac70eef84c01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5.],\n",
              "        [7., 8.]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "pool2d(X, (2, 2))"
      ],
      "id": "98aae7b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:07.629753Z",
          "iopub.status.busy": "2022-09-07T22:11:07.629125Z",
          "iopub.status.idle": "2022-09-07T22:11:07.635367Z",
          "shell.execute_reply": "2022-09-07T22:11:07.634589Z"
        },
        "origin_pos": 9,
        "tab": [
          "pytorch"
        ],
        "id": "55a43d86",
        "outputId": "1bcf7fca-1a2f-4476-85a7-83d4c2318256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 3.],\n",
              "        [5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "pool2d(X, (2, 2), 'avg')"
      ],
      "id": "55a43d86"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 10,
        "id": "c26bbf2e"
      },
      "source": [
        "## Padding y Stride\n",
        "\n",
        "Al igual que con las capas convolucionales, las capas de pooling cambian la forma de salida. Y como antes, podemos ajustar la operación para lograr la forma de salida deseada rellenando la entrada y ajustando el stride.\n",
        "Podemos demostrar el uso de padding y stride en capas de pooling a través de la capa de max pooling de `pytorch`. Primero construimos un tensor de entrada `X` cuya forma tiene cuatro dimensiones, donde el número de ejemplos (tamaño del lote) y el número de canales son ambos 1."
      ],
      "id": "c26bbf2e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:07.638719Z",
          "iopub.status.busy": "2022-09-07T22:11:07.638255Z",
          "iopub.status.idle": "2022-09-07T22:11:07.644493Z",
          "shell.execute_reply": "2022-09-07T22:11:07.643692Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "b468ea87",
        "outputId": "b12e127d-5ee5-48f6-a6d7-15aa9984aa77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  1.,  2.,  3.],\n",
              "          [ 4.,  5.,  6.,  7.],\n",
              "          [ 8.,  9., 10., 11.],\n",
              "          [12., 13., 14., 15.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
        "X"
      ],
      "id": "b468ea87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 14,
        "id": "a585edba"
      },
      "source": [
        "Como la capa de pooling agrupa ingomación de una misma area la mayoría de los frameworks supone que el tamaño de la ventana de pooling coincide con los strides. Por ejemplo, para una ventana `(3, 3)` `pytorch` supone un\n",
        "stride por defecto de  `(3, 3)`.\n"
      ],
      "id": "a585edba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:07.647898Z",
          "iopub.status.busy": "2022-09-07T22:11:07.647457Z",
          "iopub.status.idle": "2022-09-07T22:11:07.653242Z",
          "shell.execute_reply": "2022-09-07T22:11:07.652503Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "752e2e4e",
        "outputId": "dc0bb78d-85f1-48bb-8ab6-f65a14fcbb83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[10.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "pool2d = nn.MaxPool2d(3)\n",
        "# Pooling no tiene parametros no tiene inicialización\n",
        "pool2d(X)"
      ],
      "id": "752e2e4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 18,
        "id": "dd624fcb"
      },
      "source": [
        "Por supuesto, podemos modificar esto de cualquier manera que querramos."
      ],
      "id": "dd624fcb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:07.656742Z",
          "iopub.status.busy": "2022-09-07T22:11:07.656117Z",
          "iopub.status.idle": "2022-09-07T22:11:07.662536Z",
          "shell.execute_reply": "2022-09-07T22:11:07.661800Z"
        },
        "origin_pos": 20,
        "tab": [
          "pytorch"
        ],
        "id": "a80a07fc",
        "outputId": "7e08765c-84aa-49bd-f8d0-8fd511bc26f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 5.,  7.],\n",
              "          [13., 15.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
        "pool2d(X)"
      ],
      "id": "a80a07fc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:07.665880Z",
          "iopub.status.busy": "2022-09-07T22:11:07.665385Z",
          "iopub.status.idle": "2022-09-07T22:11:07.671429Z",
          "shell.execute_reply": "2022-09-07T22:11:07.670666Z"
        },
        "origin_pos": 24,
        "tab": [
          "pytorch"
        ],
        "id": "3a196b0f",
        "outputId": "6c338de2-4611-46c3-dcdf-fa758498e87f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 5.,  7.],\n",
              "          [13., 15.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
        "pool2d(X)"
      ],
      "id": "3a196b0f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 26,
        "id": "4ebea0b0"
      },
      "source": [
        "## Trabajando con múltiples canales\n",
        "\n",
        "Al procesar datos de entrada multicanal, la capa de pooling agrupa cada canal de entrada por separado, en lugar de sumar las entradas en los canales como en una capa convolucional. Esto significa que el número de canales de salida para la capa de pooling es el mismo que el número de canales de entrada. A continuación, concatenaremos los tensores `X` y `X + 1` en la dimensión del canal para construir una entrada con 2 canales."
      ],
      "id": "4ebea0b0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:07.674798Z",
          "iopub.status.busy": "2022-09-07T22:11:07.674290Z",
          "iopub.status.idle": "2022-09-07T22:11:07.680447Z",
          "shell.execute_reply": "2022-09-07T22:11:07.679688Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "c8a1f007",
        "outputId": "c6e1dfda-68de-4f80-d1ce-4aaad7aacd96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  1.,  2.,  3.],\n",
              "          [ 4.,  5.,  6.,  7.],\n",
              "          [ 8.,  9., 10., 11.],\n",
              "          [12., 13., 14., 15.]],\n",
              "\n",
              "         [[ 1.,  2.,  3.,  4.],\n",
              "          [ 5.,  6.,  7.,  8.],\n",
              "          [ 9., 10., 11., 12.],\n",
              "          [13., 14., 15., 16.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "X = torch.cat((X, X + 1), 1)\n",
        "X"
      ],
      "id": "c8a1f007"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 30,
        "id": "e56812ce"
      },
      "source": [
        "Como dijimos el número de canales de entrada es el mismo que de salido."
      ],
      "id": "e56812ce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:11:07.684218Z",
          "iopub.status.busy": "2022-09-07T22:11:07.683585Z",
          "iopub.status.idle": "2022-09-07T22:11:07.691241Z",
          "shell.execute_reply": "2022-09-07T22:11:07.690332Z"
        },
        "origin_pos": 32,
        "tab": [
          "pytorch"
        ],
        "id": "3a292a9b",
        "outputId": "90a1184b-c8f5-4420-a76c-25a2ca4d8782",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 5.,  7.],\n",
              "          [13., 15.]],\n",
              "\n",
              "         [[ 6.,  8.],\n",
              "          [14., 16.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
        "pool2d(X)"
      ],
      "id": "3a292a9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 1,
        "id": "91186d53"
      },
      "source": [
        "# LeNet\n",
        "\n",
        "Ya tenemos todos los ingredientes necesarios para armar una CNN totalmente funcional. Cuando trabajamos anteriormente con imágenes, habíamos usado\n",
        "un modelo lineal con regresión softmax y un MLP a imágenes de ropa en el dataset Fashion-MNIST. Para hacer que estos datos sean manejables, primero aplanamos cada imagen de una matriz de $28\\times28$ en un vector dimensional de $784$ de longitud fija, y luego los procesó en capas completamente conectadas. Ahora que tenemos un control sobre las capas convolucionales, podemos retener la estructura espacial en nuestras imágenes. Como beneficio adicional de reemplazar capas completamente conectadas con capas convolucionales, nuestros modelos requieren muchos menos parámetros.\n",
        "\n",
        "En esta sección, presentaremos **LeNet**, entre las primeras CNN publicadas para capturar una amplia atención por su desempeño en tareas de visión por computadora. El modelo fue presentado por (y nombrado por) Yann LeCun,\n",
        "entonces investigador en AT&T Bell Labs, con el propósito de reconocer dígitos del 0 al 9 escritos a mano en imágenes. El dataset con el que trabajó LeCun y su equipo se lo conoce como MNIST y es un modelo tan conocido y probado, que muchos especialista argumenta que carece de sentido usarlo. Por esto Fashion MNIST se ha propuesto como una alternativa. \n",
        "\n",
        "En ese momento, LeNet logró resultados sobresalientes equiparar el rendimiento de las máquinas de vectores de soporte, luego un enfoque dominante en el aprendizaje supervisado, logrando una tasa de error de menos del 1% por dígito.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## LeNet\n",
        "\n",
        "A grandes rasgos LeNet tiene dos partes. Una parte consistente de un encoder convolucional de dos capas y un MLP de 3 capas\n",
        "\n",
        "![](http://d2l.ai/_images/lenet.svg)\n",
        "\n",
        "Decimos que es un encoder convolucional, porque toma imágenes y las transforma en vectores de 120 componentes que representa la información contenida en la imágen.\n",
        "\n",
        "Las unidades básicas en cada bloque convolucional son una capa convolucional, una función de activación sigmoidea, y una posterior operación de pooling con promedios. Tenga en cuenta que, si bien las ReLU y MaxPooling funcionan mejor, estos descubrimientos aún no se habían hecho al momento de publicación de LeNet. Cada capa convolucional usa un kernel $5\\times 5$ y una función de activación sigmoidea. Observé también que a la salida de cada capa connvolucional aumenta el número de canales a la salida. La primera capa convolucional tiene 6 canales de salida, mientras que el segundo tiene 16. Cada operación de pooling $2\\times2$ (stride 2) reduce la dimensionalidad por un factor de $4$ a través de la reducción de resolución espacial. El bloque convolucional emite una salida con forma dada por (tamaño de lote, número de canal, alto, ancho).\n",
        "\n",
        "Para pasar la salida del bloque convolucional al MLP,\n",
        "debemos aplanar cada ejemplo en el minilote. En otras palabras, tomamos esta entrada de cuatro dimensiones y la transformamos en la entrada bidimensional esperada por el MLP: como recordatorio, este tensor tiene la forma `[tamaño de minilote, dim plana]`. En número `dim plana` es el número de dimensiones del vector utilizado para la representación vectorial plana de cada ejemplo El bloque denso de LeNet tiene tres capas completamente conectadas, con 120, 84 y 10 salidas, respectivamente.\n",
        "Debido a que todavía estamos realizando la clasificación, la capa de salida de 10 dimensiones corresponde al número de posibles clases de salida.\n",
        "\n",
        "A pesar de lo tortuoso que puede parecer la descripción anterior, la implementación usando `torch` es sencilla y no necesitaremos nada más que un método `Sequential`\n"
      ],
      "id": "91186d53"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:36:15.531121Z",
          "iopub.status.busy": "2022-09-07T22:36:15.530759Z",
          "iopub.status.idle": "2022-09-07T22:36:17.530130Z",
          "shell.execute_reply": "2022-09-07T22:36:17.528931Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "6af9f4db"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n"
      ],
      "id": "6af9f4db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:36:17.534765Z",
          "iopub.status.busy": "2022-09-07T22:36:17.534136Z",
          "iopub.status.idle": "2022-09-07T22:36:17.539915Z",
          "shell.execute_reply": "2022-09-07T22:36:17.538823Z"
        },
        "origin_pos": 5,
        "tab": [
          "pytorch"
        ],
        "id": "500f8d90"
      },
      "outputs": [],
      "source": [
        "def init_cnn(module):\n",
        "    \"\"\"Initialize weights for CNNs.\"\"\"\n",
        "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
        "        nn.init.xavier_uniform_(module.weight)"
      ],
      "id": "500f8d90"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:36:17.543613Z",
          "iopub.status.busy": "2022-09-07T22:36:17.543143Z",
          "iopub.status.idle": "2022-09-07T22:36:17.549868Z",
          "shell.execute_reply": "2022-09-07T22:36:17.549016Z"
        },
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "b845b46b"
      },
      "outputs": [],
      "source": [
        "NUM_CHANNEL1 = 6\n",
        "NUM_CHANNEL2 = 16\n",
        "NUM_MLP1 = 120\n",
        "NUM_MLP2 = 84\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "model = nn.Sequential(\n",
        "            nn.LazyConv2d(NUM_CHANNEL1, kernel_size=5, padding=2), nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(NUM_CHANNEL2, kernel_size=5), nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(NUM_MLP1), nn.Sigmoid(),\n",
        "            nn.LazyLinear(NUM_MLP2), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))"
      ],
      "id": "b845b46b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 7,
        "id": "2576321b"
      },
      "source": [
        "Para aquellos que quieran analizar el paper de LeCunn, podrán ver que solo hemos hecho una pequeña modificación. LeCunn y su equipo usaron un clasificación gaussiano, mientras que nosotros usaremos SoftMax como lo hemos hecho hasta ahora. \n",
        "\n",
        "\n",
        "![](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/lenet-vert.svg?raw=1)\n"
      ],
      "id": "2576321b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación mostramos como cambia la forma del tensor entrante y saliente con cada una de las capas usadas."
      ],
      "metadata": {
        "id": "Ar_o7cO0kT9a"
      },
      "id": "Ar_o7cO0kT9a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:36:17.553576Z",
          "iopub.status.busy": "2022-09-07T22:36:17.553152Z",
          "iopub.status.idle": "2022-09-07T22:36:17.596538Z",
          "shell.execute_reply": "2022-09-07T22:36:17.595333Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "c0d44c17",
        "outputId": "4a471a5f-d5ae-48a8-91c4-0065d9553352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrada original:\t torch.Size([1, 1, 28, 28])\n",
            "Salida tras Conv2d:\t torch.Size([1, 6, 28, 28])\n",
            "Salida tras Sigmoid:\t torch.Size([1, 6, 28, 28])\n",
            "Salida tras AvgPool2d:\t torch.Size([1, 6, 14, 14])\n",
            "Salida tras Conv2d:\t torch.Size([1, 16, 10, 10])\n",
            "Salida tras Sigmoid:\t torch.Size([1, 16, 10, 10])\n",
            "Salida tras AvgPool2d:\t torch.Size([1, 16, 5, 5])\n",
            "Salida tras Flatten:\t torch.Size([1, 400])\n",
            "Salida tras Linear:\t torch.Size([1, 120])\n",
            "Salida tras Sigmoid:\t torch.Size([1, 120])\n",
            "Salida tras Linear:\t torch.Size([1, 84])\n",
            "Salida tras Sigmoid:\t torch.Size([1, 84])\n",
            "Salida tras Linear:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "def layer_summary(net, X_shape):\n",
        "    X = torch.randn(*X_shape)\n",
        "    print(\"Entrada original:\\t\", X.shape)\n",
        "    for layer in net:\n",
        "        X = layer(X)\n",
        "        print(\"Salida tras \"+layer.__class__.__name__+':\\t', X.shape)\n",
        "\n",
        "layer_summary(model, (1, 1, 28, 28))"
      ],
      "id": "c0d44c17"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 10,
        "id": "56cc32df"
      },
      "source": [
        "## Cargando los datos.\n",
        "\n",
        "Como dijimos, vamos a trabajar con Fashion MNIST. Para ello cargaremos le dataset desde la biblioteca de torch."
      ],
      "id": "56cc32df"
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils import data\n",
        "\n",
        "def load_data_fashion_mnist(batch_size):\n",
        "    trans = [transforms.ToTensor()]\n",
        "    trans = transforms.Compose(trans)\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=True, transform=trans, download=True)\n",
        "    length = len(mnist_train)\n",
        "    stop = int(len(mnist_train) * 0.7)\n",
        "    mnist_val = [mnist_train[i] for i in range(stop,length)]\n",
        "    mnist_train = [mnist_train[i] for i in range(stop)]\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=False, transform=trans, download=True)\n",
        "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
        "                            num_workers=1),\n",
        "            data.DataLoader(mnist_val, batch_size, shuffle=True,\n",
        "                            num_workers=1),\n",
        "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
        "                            num_workers=1))\n",
        "\n",
        "batch_size = 1024\n",
        "iter_train, iter_val, iter_test = load_data_fashion_mnist(batch_size)\n"
      ],
      "metadata": {
        "id": "e_RXGdi4ttbZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448,
          "referenced_widgets": [
            "2f72c80e1ba0440daf88464961807657",
            "546c1b77a6ce49f5acf61f63e187dca8",
            "4f1f5640564c41b0b305ad758c311a40",
            "82e56cb7961248e786cb365bdb9b9c36",
            "1db91ca9361f40e08cf32f20c86d2ff2",
            "d461cf409d0e4e0baa622609ce4f0779",
            "d256712046be4c53bc0f05245c59b674",
            "bbbad284f5b344ae8a0674ccf50945d5",
            "fc3caa1e34b8425898637b2e48070502",
            "cd5f8c868e9b4e4690d80f6cebc1ac9a",
            "865b8e74ae1d4895b03d9208c07c1d8f",
            "94a7b85f0795467e941323de19b995f8",
            "fd9c1588d7ab434ba423b6c3a8076986",
            "bd6002bac22a4e9f99c6209f23bdd0d8",
            "3cefb5f511e2408593ceb1ca9b7962c0",
            "6fd51adf68214678930506763056fc99",
            "c6814101da2e4274b4ba13f4110feb89",
            "5f05dbab242241e9aecf219fa91b973f",
            "35b8843745c44458be1fe4f7ccdaca67",
            "8dce10caded94861a2ddf08f834ff20f",
            "9540b6ab4d2d450fb8d4b2641b4cadd6",
            "8089c6f0b7dc45619d19329558224298",
            "6f339d1910fc4861b2abe858ad63cdba",
            "0c09047511dd453d8d06f09865c107fa",
            "f14430c1ea474d458201f1bc6a62b8e8",
            "404390e360eb43d48d41401362ef6eb7",
            "401711f7fa0e4dee8d0df08b5f50b2f5",
            "47084aabc25148c0b852cbf95a982f36",
            "e38e9abd5c2041359c14bc56fa6de3d4",
            "968ad41ad8584b2885bea9f87e893a1b",
            "79226accfa8f4ae1935d4b66b5221af4",
            "da0cb4d10c5640bfabbc4f75449285b3",
            "ed6ccacf476b4adb9d5b93ae6251d0a2",
            "ea4545997ab6479e87693a13bf301c66",
            "dbed383b7f224607bfb28806591e6413",
            "8811e582e51741c088459d62fd838c69",
            "f532af3ea1594bad8c66e3326eb6bb72",
            "0bdb8865acbb4d8f823441423b5e92e7",
            "8872b96560c1489590aaef25984cad2a",
            "4fcdf224eaaa4b59a0955eca79ebf700",
            "77778d7ede0245f2bb20cf2e0339f284",
            "e8357a7011d746ecbaf096d388f9df8c",
            "7e2f94a94b984acd8b9b095d2cd0e1f0",
            "6220f72c54b44223a5d58c1bec1c0a59"
          ]
        },
        "outputId": "6be69c07-0133-435c-8475-8913575838a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/26421880 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f72c80e1ba0440daf88464961807657"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/29515 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94a7b85f0795467e941323de19b995f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4422102 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f339d1910fc4861b2abe858ad63cdba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea4545997ab6479e87693a13bf301c66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "id": "e_RXGdi4ttbZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "También calcularemos el accuracy de nuestro modelo."
      ],
      "metadata": {
        "id": "RHQAvElBnG0F"
      },
      "id": "RHQAvElBnG0F"
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_accuracy(preds, y):\n",
        "\n",
        "    # aproximamos al entero más cercano\n",
        "    preds = torch.argmax(preds, dim=1)\n",
        "    correct = (preds == y).float() #convertimos a flotante para la división\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "55lh8pUWwrGJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "55lh8pUWwrGJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definiremos una función de entrenamiento y evaluación como las que habíamos usado antes."
      ],
      "metadata": {
        "id": "m3x5g4YJnUrR"
      },
      "id": "m3x5g4YJnUrR"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    \n",
        "    for batch in iterator:\n",
        "\n",
        "        image, label = batch\n",
        "        image, label  = image.to(device), label.to(device)      \n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        acc = binary_accuracy(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "Z-lJWjrusi6k"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Z-lJWjrusi6k"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion, device):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "\n",
        "      for batch in iterator:\n",
        "\n",
        "          image, label = batch\n",
        "          image, label  = image.to(device), label.to(device)      \n",
        "          optimizer.zero_grad()\n",
        "          output = model(image)\n",
        "          loss = criterion(output, label)\n",
        "          acc = binary_accuracy(output, label)\n",
        "          epoch_loss += loss.item()\n",
        "          epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "JxH4-DPyxOW-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JxH4-DPyxOW-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "y una función para calcular el tiempo de cálculo"
      ],
      "metadata": {
        "id": "0oJBb5OIn7pc"
      },
      "id": "0oJBb5OIn7pc"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "5BNJAZKbxyTr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5BNJAZKbxyTr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Entrenamiento"
      ],
      "metadata": {
        "id": "yoI6skXundge"
      },
      "id": "yoI6skXundge"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "0_u0iI3Cx4Fn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0_u0iI3Cx4Fn"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, iter_train, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(model, iter_val, criterion, device)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train acc.: {train_acc:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. acc.: {valid_acc:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrXqxmgiyBeE",
        "outputId": "9da24d38-722e-42ae-cf24-bb655be305eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 11s\n",
            "\tTrain Loss: 2.306 | Train acc.: 0.099\n",
            "\t Val. Loss: 2.303 |  Val. acc.: 0.097\n",
            "Epoch: 02 | Time: 0m 10s\n",
            "\tTrain Loss: 2.274 | Train acc.: 0.173\n",
            "\t Val. Loss: 2.168 |  Val. acc.: 0.316\n",
            "Epoch: 03 | Time: 0m 10s\n",
            "\tTrain Loss: 1.804 | Train acc.: 0.388\n",
            "\t Val. Loss: 1.446 |  Val. acc.: 0.508\n",
            "Epoch: 04 | Time: 0m 10s\n",
            "\tTrain Loss: 1.283 | Train acc.: 0.537\n",
            "\t Val. Loss: 1.159 |  Val. acc.: 0.573\n",
            "Epoch: 05 | Time: 0m 10s\n",
            "\tTrain Loss: 1.101 | Train acc.: 0.581\n",
            "\t Val. Loss: 1.033 |  Val. acc.: 0.599\n",
            "Epoch: 06 | Time: 0m 10s\n",
            "\tTrain Loss: 1.005 | Train acc.: 0.602\n",
            "\t Val. Loss: 0.964 |  Val. acc.: 0.621\n",
            "Epoch: 07 | Time: 0m 11s\n",
            "\tTrain Loss: 0.944 | Train acc.: 0.624\n",
            "\t Val. Loss: 0.920 |  Val. acc.: 0.630\n",
            "Epoch: 08 | Time: 0m 10s\n",
            "\tTrain Loss: 0.907 | Train acc.: 0.635\n",
            "\t Val. Loss: 0.886 |  Val. acc.: 0.651\n",
            "Epoch: 09 | Time: 0m 10s\n",
            "\tTrain Loss: 0.873 | Train acc.: 0.659\n",
            "\t Val. Loss: 0.853 |  Val. acc.: 0.657\n",
            "Epoch: 10 | Time: 0m 10s\n",
            "\tTrain Loss: 0.860 | Train acc.: 0.669\n",
            "\t Val. Loss: 0.821 |  Val. acc.: 0.687\n",
            "Epoch: 11 | Time: 0m 10s\n",
            "\tTrain Loss: 0.811 | Train acc.: 0.694\n",
            "\t Val. Loss: 0.796 |  Val. acc.: 0.695\n",
            "Epoch: 12 | Time: 0m 10s\n",
            "\tTrain Loss: 0.790 | Train acc.: 0.704\n",
            "\t Val. Loss: 0.786 |  Val. acc.: 0.699\n",
            "Epoch: 13 | Time: 0m 10s\n",
            "\tTrain Loss: 0.770 | Train acc.: 0.711\n",
            "\t Val. Loss: 0.755 |  Val. acc.: 0.721\n",
            "Epoch: 14 | Time: 0m 11s\n",
            "\tTrain Loss: 0.772 | Train acc.: 0.709\n",
            "\t Val. Loss: 0.744 |  Val. acc.: 0.714\n",
            "Epoch: 15 | Time: 0m 11s\n",
            "\tTrain Loss: 0.730 | Train acc.: 0.722\n",
            "\t Val. Loss: 0.717 |  Val. acc.: 0.724\n",
            "Epoch: 16 | Time: 0m 11s\n",
            "\tTrain Loss: 0.713 | Train acc.: 0.728\n",
            "\t Val. Loss: 0.703 |  Val. acc.: 0.729\n",
            "Epoch: 17 | Time: 0m 10s\n",
            "\tTrain Loss: 0.693 | Train acc.: 0.736\n",
            "\t Val. Loss: 0.687 |  Val. acc.: 0.738\n",
            "Epoch: 18 | Time: 0m 10s\n",
            "\tTrain Loss: 0.674 | Train acc.: 0.743\n",
            "\t Val. Loss: 0.666 |  Val. acc.: 0.742\n",
            "Epoch: 19 | Time: 0m 11s\n",
            "\tTrain Loss: 0.659 | Train acc.: 0.745\n",
            "\t Val. Loss: 0.654 |  Val. acc.: 0.742\n",
            "Epoch: 20 | Time: 0m 10s\n",
            "\tTrain Loss: 0.649 | Train acc.: 0.747\n",
            "\t Val. Loss: 0.639 |  Val. acc.: 0.753\n"
          ]
        }
      ],
      "id": "OrXqxmgiyBeE"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, iter_test, criterion ,device)\n",
        "\n",
        "print(f'\\t Test. loss: {test_loss:.3f} |  test. acc: {test_acc:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a-3dNLjyRtl",
        "outputId": "af309286-1df6-4469-c4d7-2453f92e14d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Test. loss: 0.665 |  test. acc: 0.747\n"
          ]
        }
      ],
      "id": "5a-3dNLjyRtl"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f72c80e1ba0440daf88464961807657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_546c1b77a6ce49f5acf61f63e187dca8",
              "IPY_MODEL_4f1f5640564c41b0b305ad758c311a40",
              "IPY_MODEL_82e56cb7961248e786cb365bdb9b9c36"
            ],
            "layout": "IPY_MODEL_1db91ca9361f40e08cf32f20c86d2ff2"
          }
        },
        "546c1b77a6ce49f5acf61f63e187dca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d461cf409d0e4e0baa622609ce4f0779",
            "placeholder": "​",
            "style": "IPY_MODEL_d256712046be4c53bc0f05245c59b674",
            "value": "100%"
          }
        },
        "4f1f5640564c41b0b305ad758c311a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbbad284f5b344ae8a0674ccf50945d5",
            "max": 26421880,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc3caa1e34b8425898637b2e48070502",
            "value": 26421880
          }
        },
        "82e56cb7961248e786cb365bdb9b9c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd5f8c868e9b4e4690d80f6cebc1ac9a",
            "placeholder": "​",
            "style": "IPY_MODEL_865b8e74ae1d4895b03d9208c07c1d8f",
            "value": " 26421880/26421880 [00:01&lt;00:00, 27475847.50it/s]"
          }
        },
        "1db91ca9361f40e08cf32f20c86d2ff2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d461cf409d0e4e0baa622609ce4f0779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d256712046be4c53bc0f05245c59b674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbbad284f5b344ae8a0674ccf50945d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc3caa1e34b8425898637b2e48070502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd5f8c868e9b4e4690d80f6cebc1ac9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "865b8e74ae1d4895b03d9208c07c1d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94a7b85f0795467e941323de19b995f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd9c1588d7ab434ba423b6c3a8076986",
              "IPY_MODEL_bd6002bac22a4e9f99c6209f23bdd0d8",
              "IPY_MODEL_3cefb5f511e2408593ceb1ca9b7962c0"
            ],
            "layout": "IPY_MODEL_6fd51adf68214678930506763056fc99"
          }
        },
        "fd9c1588d7ab434ba423b6c3a8076986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6814101da2e4274b4ba13f4110feb89",
            "placeholder": "​",
            "style": "IPY_MODEL_5f05dbab242241e9aecf219fa91b973f",
            "value": "100%"
          }
        },
        "bd6002bac22a4e9f99c6209f23bdd0d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35b8843745c44458be1fe4f7ccdaca67",
            "max": 29515,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dce10caded94861a2ddf08f834ff20f",
            "value": 29515
          }
        },
        "3cefb5f511e2408593ceb1ca9b7962c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9540b6ab4d2d450fb8d4b2641b4cadd6",
            "placeholder": "​",
            "style": "IPY_MODEL_8089c6f0b7dc45619d19329558224298",
            "value": " 29515/29515 [00:00&lt;00:00, 290829.75it/s]"
          }
        },
        "6fd51adf68214678930506763056fc99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6814101da2e4274b4ba13f4110feb89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f05dbab242241e9aecf219fa91b973f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35b8843745c44458be1fe4f7ccdaca67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dce10caded94861a2ddf08f834ff20f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9540b6ab4d2d450fb8d4b2641b4cadd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8089c6f0b7dc45619d19329558224298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f339d1910fc4861b2abe858ad63cdba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c09047511dd453d8d06f09865c107fa",
              "IPY_MODEL_f14430c1ea474d458201f1bc6a62b8e8",
              "IPY_MODEL_404390e360eb43d48d41401362ef6eb7"
            ],
            "layout": "IPY_MODEL_401711f7fa0e4dee8d0df08b5f50b2f5"
          }
        },
        "0c09047511dd453d8d06f09865c107fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47084aabc25148c0b852cbf95a982f36",
            "placeholder": "​",
            "style": "IPY_MODEL_e38e9abd5c2041359c14bc56fa6de3d4",
            "value": "100%"
          }
        },
        "f14430c1ea474d458201f1bc6a62b8e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_968ad41ad8584b2885bea9f87e893a1b",
            "max": 4422102,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79226accfa8f4ae1935d4b66b5221af4",
            "value": 4422102
          }
        },
        "404390e360eb43d48d41401362ef6eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da0cb4d10c5640bfabbc4f75449285b3",
            "placeholder": "​",
            "style": "IPY_MODEL_ed6ccacf476b4adb9d5b93ae6251d0a2",
            "value": " 4422102/4422102 [00:00&lt;00:00, 9324445.97it/s]"
          }
        },
        "401711f7fa0e4dee8d0df08b5f50b2f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47084aabc25148c0b852cbf95a982f36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38e9abd5c2041359c14bc56fa6de3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "968ad41ad8584b2885bea9f87e893a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79226accfa8f4ae1935d4b66b5221af4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da0cb4d10c5640bfabbc4f75449285b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed6ccacf476b4adb9d5b93ae6251d0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea4545997ab6479e87693a13bf301c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbed383b7f224607bfb28806591e6413",
              "IPY_MODEL_8811e582e51741c088459d62fd838c69",
              "IPY_MODEL_f532af3ea1594bad8c66e3326eb6bb72"
            ],
            "layout": "IPY_MODEL_0bdb8865acbb4d8f823441423b5e92e7"
          }
        },
        "dbed383b7f224607bfb28806591e6413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8872b96560c1489590aaef25984cad2a",
            "placeholder": "​",
            "style": "IPY_MODEL_4fcdf224eaaa4b59a0955eca79ebf700",
            "value": "100%"
          }
        },
        "8811e582e51741c088459d62fd838c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77778d7ede0245f2bb20cf2e0339f284",
            "max": 5148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8357a7011d746ecbaf096d388f9df8c",
            "value": 5148
          }
        },
        "f532af3ea1594bad8c66e3326eb6bb72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2f94a94b984acd8b9b095d2cd0e1f0",
            "placeholder": "​",
            "style": "IPY_MODEL_6220f72c54b44223a5d58c1bec1c0a59",
            "value": " 5148/5148 [00:00&lt;00:00, 108588.48it/s]"
          }
        },
        "0bdb8865acbb4d8f823441423b5e92e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8872b96560c1489590aaef25984cad2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fcdf224eaaa4b59a0955eca79ebf700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77778d7ede0245f2bb20cf2e0339f284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8357a7011d746ecbaf096d388f9df8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e2f94a94b984acd8b9b095d2cd0e1f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6220f72c54b44223a5d58c1bec1c0a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}